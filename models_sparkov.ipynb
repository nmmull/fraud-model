{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18577425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/kartik2112/fraud-detection?select=fraudTrain.csv\n",
    "# dataset preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_train = pd.read_csv(\"fraudTrain.csv\")\n",
    "df_test = pd.read_csv(\"fraudTest.csv\")\n",
    "\n",
    "# data preprocessing plus exploratory data analysis\n",
    "def preprocess(df):\n",
    "    # combine first and last name to one column\n",
    "    #df['name'] = df['first'] + df['last']\n",
    "    df = df.drop(columns=['first','last'])\n",
    "\n",
    "    # combine address into\n",
    "\n",
    "    # drop time, also drop data for a baseline test (can manipulate what data to add back in later)\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df = df.sort_values(by=\"trans_date_trans_time\", ascending=True)\n",
    "    df = df.drop(columns=['trans_date_trans_time', 'job', 'dob', 'unix_time', 'city_pop', 'category', 'street', 'city', 'state', 'zip'\n",
    "                          , 'gender', 'cc_num', 'trans_num'])\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = preprocess(df_train)\n",
    "df_test = preprocess(df_test)\n",
    "\n",
    "# training data\n",
    "X_training = df_train.drop('is_fraud', axis=1)\n",
    "y_training = df_train['is_fraud']\n",
    "\n",
    "# testing data\n",
    "X_test = df_test.drop('is_fraud', axis=1)\n",
    "y_test = df_test['is_fraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding for categorical data\n",
    "ohe_merchant = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_merchant.fit(df_train[['merchant']])\n",
    "X_training_ohe= ohe_merchant.transform(X_training[['merchant']])\n",
    "X_testing_ohe = ohe_merchant.transform(X_test[['merchant']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e106a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       legit       1.00      0.55      0.71    553574\n",
      "       fraud       0.01      0.77      0.01      2145\n",
      "\n",
      "    accuracy                           0.55    555719\n",
      "   macro avg       0.50      0.66      0.36    555719\n",
      "weighted avg       0.99      0.55      0.71    555719\n",
      "\n",
      "PRAUC:  0.00910538255392279\n",
      "              Predicted Legit  Predicted Fraud\n",
      "Actual Legit           304399           249175\n",
      "Actual Fraud              498             1647\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# use balanced to let the model increase weighting for fraud cases\n",
    "logistic_regression = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "logistic_regression.fit(X_training_ohe, y_training)\n",
    "lr_pred = logistic_regression.predict(X_testing_ohe)\n",
    "\n",
    "# manipulating the threshold (threshold here is set to get some amount in each category of confusion matrix)\n",
    "lr_prob = logistic_regression.predict_proba(X_testing_ohe)[:, 1]\n",
    "threshold = 0.4\n",
    "lr_pred = (lr_prob >= threshold).astype(int)\n",
    "\n",
    "# print results\n",
    "target_names = [\"legit\", \"fraud\"]\n",
    "print(classification_report(y_test, lr_pred, target_names=target_names, zero_division=0))\n",
    "print(\"PRAUC: \", average_precision_score(y_test, lr_prob))\n",
    "\n",
    "cm = confusion_matrix(y_test,lr_pred)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual Legit\", \"Actual Fraud\"],\n",
    "    columns=[\"Predicted Legit\", \"Predicted Fraud\"]\n",
    ")\n",
    "\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da464db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       legit       1.00      0.76      0.86    553574\n",
      "       fraud       0.01      0.60      0.02      2145\n",
      "\n",
      "    accuracy                           0.76    555719\n",
      "   macro avg       0.50      0.68      0.44    555719\n",
      "weighted avg       0.99      0.76      0.86    555719\n",
      "\n",
      "PRAUC:  0.009063840551307855\n",
      "              Predicted Legit  Predicted Fraud\n",
      "Actual Legit           421531           132043\n",
      "Actual Fraud              850             1295\n"
     ]
    }
   ],
   "source": [
    "# SVM model\n",
    "# have to use linearSVM because SVM scales horribly with this dataset\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "svm = LinearSVC(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=10000,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "svm.fit(X_training_ohe, y_training)\n",
    "svm_pred = svm.predict(X_testing_ohe)\n",
    "\n",
    "# manipulating the threshold (threshold here is set to get some amount in each category of confusion matrix)\n",
    "decision_scores = svm.decision_function(X_testing_ohe)\n",
    "threshold = 0.2 \n",
    "svm_pred = (decision_scores >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, svm_pred, target_names=target_names, zero_division=0))\n",
    "print(\"PRAUC: \", average_precision_score(y_test, decision_scores))\n",
    "\n",
    "cm_svm = confusion_matrix(y_test, svm_pred)\n",
    "cm_svm_df = pd.DataFrame(\n",
    "    cm_svm,\n",
    "    index=[\"Actual Legit\", \"Actual Fraud\"],\n",
    "    columns=[\"Predicted Legit\", \"Predicted Fraud\"]\n",
    ")\n",
    "\n",
    "print(cm_svm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d678210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       legit       1.00      0.79      0.88    553574\n",
      "       fraud       0.01      0.56      0.02      2145\n",
      "\n",
      "    accuracy                           0.79    555719\n",
      "   macro avg       0.50      0.67      0.45    555719\n",
      "weighted avg       0.99      0.79      0.88    555719\n",
      "\n",
      "PRAUC:  0.009030535032463377\n",
      "              Predicted Legit  Predicted Fraud\n",
      "Actual Legit           435305           118269\n",
      "Actual Fraud              938             1207\n"
     ]
    }
   ],
   "source": [
    "# neural network (multilayer perceptron) (~2 min to run)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# baseline hyperparameters,\n",
    "mlp = make_pipeline(StandardScaler(with_mean=False), MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64), # larger dataset requires more nodes in each layer\n",
    "    activation='relu',            \n",
    "    solver='adam',\n",
    "    alpha = 0.001,               \n",
    "    max_iter=500,        \n",
    "    random_state=0,\n",
    "))\n",
    "\n",
    "mlp.fit(X_training_ohe, y_training)\n",
    "mlp_pred = mlp.predict(X_testing_ohe)\n",
    "\n",
    "mlp_prob = mlp.predict_proba(X_testing_ohe)[:, 1]\n",
    "\n",
    "# manipulating the threshold (threshold here is set to get some amount in each category of confusion matrix)\n",
    "# 0.05 and above results in model not finding anything\n",
    "threshold = 0.01\n",
    "mlp_pred = (mlp_prob >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, mlp_pred, target_names=target_names, zero_division=0))\n",
    "print(\"PRAUC: \", average_precision_score(y_test, mlp_prob))\n",
    "\n",
    "cm_mlp = confusion_matrix(y_test, mlp_pred)\n",
    "cm_mlp_df = pd.DataFrame(\n",
    "    cm_mlp,\n",
    "    index=[\"Actual Legit\", \"Actual Fraud\"],\n",
    "    columns=[\"Predicted Legit\", \"Predicted Fraud\"]\n",
    ")\n",
    "\n",
    "print(cm_mlp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c19aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       legit       1.00      0.19      0.33    553574\n",
      "       fraud       0.00      0.93      0.01      2145\n",
      "\n",
      "    accuracy                           0.20    555719\n",
      "   macro avg       0.50      0.56      0.17    555719\n",
      "weighted avg       0.99      0.20      0.32    555719\n",
      "\n",
      "PRAUC:  0.008352688758024767\n",
      "              Predicted Legit  Predicted Fraud\n",
      "Actual Legit           107595           445979\n",
      "Actual Fraud              151             1994\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "num_pos = y_training.sum()\n",
    "num_neg = len(y_training) - num_pos\n",
    "xgb = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=500,  \n",
    "    max_depth=5,        \n",
    "    learning_rate=0.05, \n",
    "    subsample=0.8,      \n",
    "    colsample_bytree=0.8, \n",
    "    random_state=0,\n",
    "    tree_method='hist', \n",
    "    scale_pos_weight=num_neg /num_pos, # balance the classes to have more weight on fraud cases\n",
    ")\n",
    "\n",
    "xgb.fit(X_training_ohe, y_training)\n",
    "xgb_pred = xgb.predict(X_testing_ohe)\n",
    "\n",
    "xgb_prob = xgb.predict_proba(X_testing_ohe)[:, 1]\n",
    "\n",
    "# manipulating the threshold (threshold here is set to get some amount in each category of confusion matrix)\n",
    "threshold = 0.4\n",
    "xgb_pred = (xgb_prob >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, xgb_pred, target_names=target_names, zero_division=0))\n",
    "print(\"PRAUC: \", average_precision_score(y_test, xgb_prob))\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, xgb_pred)\n",
    "cm_xgb_df = pd.DataFrame(\n",
    "    cm_xgb,\n",
    "    index=[\"Actual Legit\", \"Actual Fraud\"],\n",
    "    columns=[\"Predicted Legit\", \"Predicted Fraud\"]\n",
    ")\n",
    "\n",
    "print(cm_xgb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c88a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       legit       1.00      0.01      0.01    553574\n",
      "       fraud       0.00      1.00      0.01      2145\n",
      "\n",
      "    accuracy                           0.01    555719\n",
      "   macro avg       0.50      0.50      0.01    555719\n",
      "weighted avg       0.99      0.01      0.01    555719\n",
      "\n",
      "PRAUC:  0.008063305459718513\n",
      "              Predicted Legit  Predicted Fraud\n",
      "Actual Legit             4161           549413\n",
      "Actual Fraud                5             2140\n"
     ]
    }
   ],
   "source": [
    "# Random Forest (~1-2 min)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500, \n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    max_depth=15\n",
    ")\n",
    "\n",
    "# baseline test\n",
    "rf.fit(X_training_ohe, y_training)\n",
    "rf_pred = rf.predict(X_testing_ohe)\n",
    "\n",
    "# manipulating the threshold (threshold here is set to get some amount in each category of confusion matrix)\n",
    "rf_prob = rf.predict_proba(X_testing_ohe)[:, 1]\n",
    "threshold = 0.45\n",
    "rf_pred = (rf_prob >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, rf_pred, target_names=target_names, zero_division=0))\n",
    "print(\"PRAUC: \", average_precision_score(y_test, rf_prob))\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, rf_pred)\n",
    "cm_rf_df = pd.DataFrame(\n",
    "    cm_rf,\n",
    "    index=[\"Actual Legit\", \"Actual Fraud\"],\n",
    "    columns=[\"Predicted Legit\", \"Predicted Fraud\"]\n",
    ")\n",
    "\n",
    "print(cm_rf_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
